{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding MB interactions with Graph CNN-simulated data all scenarios2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMSGz17GCoLD01DzMbZ2GQp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuanwxu/corr-net-classify/blob/main/Understanding_MB_interactions_with_Graph_CNN_simulated_data_all_scenarios2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85WHdLQo2cES"
      },
      "source": [
        "## Run multiple iterations of DGCNN\n",
        "\n",
        "\n",
        "\n",
        "*   In each iteration, a different training/validation set is sampled and DGCNN model fitted.\n",
        "*   When computing node importance, use a random subset of (positive) graphs, then aggregate results from all iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPSeCYND2I-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04051f32-86a0-4ed8-8282-14d956958026"
      },
      "source": [
        "# install StellarGraph if running on Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  %pip install -q stellargraph[demos]==1.2.1\n",
        "\n",
        "# verify that we're using the correct version of StellarGraph for this notebook\n",
        "import stellargraph as sg\n",
        "\n",
        "try:\n",
        "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
        "except AttributeError:\n",
        "    raise ValueError(\n",
        "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
        "    ) from None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 440kB 26.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 39.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25h  Building wheel for mplleaflet (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcZze6R92UwD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import DeepGraphCNN\n",
        "from stellargraph import StellarGraph\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
        "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A54WzK_v2Xs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d04ae6-d1a5-4e07-ab8d-0624ec07409e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK7OxMoNCQkQ"
      },
      "source": [
        "def read_graphs2(W, node_features=None):\n",
        "  \"\"\"Read graphs into list of StellarGraph instances\n",
        "     Args:\n",
        "          W: dataframe of graphs with 4 columns: graph_ind, source, target, weight\n",
        "  \"\"\"\n",
        "  out = list()\n",
        "  if node_features:\n",
        "    for _,g in W.groupby('graph_ind'):\n",
        "      out.append(StellarGraph(nodes=node_features,     \n",
        "                              edges=g.drop(columns='graph_ind'),\n",
        "                              node_type_default='microbe',\n",
        "                              edge_type_default='correlation'))  \n",
        "  else:\n",
        "    for _,g in W.groupby('graph_ind'):\n",
        "      out.append(StellarGraph(edges=g.drop(columns='graph_ind'),\n",
        "                              node_type_default='microbe',\n",
        "                              edge_type_default='correlation'))  \n",
        "  # Check all graphs have the same number of nodes\n",
        "  nn = [g.number_of_nodes() for g in out]\n",
        "  if not all(nn[0] == x for x in nn):\n",
        "      raise ValueError(\"Not all graphs have same number of nodes, possibly incorrect chunk size.\")\n",
        "  return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG0zz7QZStWb"
      },
      "source": [
        "def train_dgcnn(graphs, graph_labels, n_epochs=50):\n",
        "  \"\"\" Build and train DGCNN model for input graphs \"\"\"\n",
        "  generator = PaddedGraphGenerator(graphs=graphs)\n",
        "  k = graphs[0].number_of_nodes()  # the number of rows for the output tensor, no truncation\n",
        "                                 # done here because all graphs have same number of nodes\n",
        "  layer_sizes = [32, 32, 32, 1]\n",
        "\n",
        "  dgcnn_model = DeepGraphCNN(\n",
        "      layer_sizes=layer_sizes,\n",
        "      activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
        "      k=k,\n",
        "      bias=False,\n",
        "      generator=generator,\n",
        "  )\n",
        "  x_inp, x_out = dgcnn_model.in_out_tensors()\n",
        "  x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
        "  x_out = MaxPool1D(pool_size=2)(x_out)\n",
        "  x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
        "  x_out = Flatten()(x_out)\n",
        "  x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
        "  x_out = Dropout(rate=0.5)(x_out)\n",
        "  predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n",
        "  model = Model(inputs=x_inp, outputs=predictions)\n",
        "  model.compile(\n",
        "      optimizer=Adam(learning_rate=0.0001), loss=binary_crossentropy, metrics=[\"acc\"],\n",
        "  )\n",
        "\n",
        "  train_graphs, test_graphs = model_selection.train_test_split(\n",
        "    graph_labels, train_size=0.8, test_size=None, stratify=graph_labels\n",
        "  )\n",
        "\n",
        "  gen = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "  # if use symmetric normalization, problem arise in negative degree values (because \n",
        "  # graph can have negative weights), and so can't take square root of those.\n",
        "  train_gen = gen.flow(\n",
        "      list(train_graphs.index),\n",
        "      targets=train_graphs.values,\n",
        "      batch_size=20,\n",
        "      symmetric_normalization=False, \n",
        "      weighted=True,\n",
        "  )\n",
        "\n",
        "  test_gen = gen.flow(\n",
        "      list(test_graphs.index),\n",
        "      targets=test_graphs.values,\n",
        "      batch_size=1,\n",
        "      symmetric_normalization=False,\n",
        "      weighted=True,\n",
        "  )\n",
        "\n",
        "  history = model.fit(\n",
        "    train_gen, epochs=n_epochs, verbose=0, validation_data=test_gen, shuffle=True\n",
        "  )\n",
        "  # Print test set metrics\n",
        "  test_metrics = model.evaluate(test_gen, verbose=0)\n",
        "  print(f'Test Set Metrics for: ')\n",
        "  for name, val in zip(model.metrics_names, test_metrics):\n",
        "      print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "  \n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwjb4pGPGTst"
      },
      "source": [
        "class ImportanceDGCNN:  \n",
        "\n",
        "  def __init__(self, W, node_features, model):\n",
        "    \"\"\"Initialize object for computing node/edge importance in predicting positive/negative class\n",
        "       resulting from the DGCNN graph classification model\n",
        "       Args:\n",
        "            W: dataframe of graphs with 4 columns: graph_ind, source, target, weight\n",
        "            node_features: used to build StellarGraph graph instance, same as read_graphs\n",
        "            model: the trained keras model of DGCNN\n",
        "    \"\"\"\n",
        "    self._W = W\n",
        "    self._node_features = node_features\n",
        "\n",
        "    # Check if all graphs have same number of edges\n",
        "    gsize = W.groupby('graph_ind').size() \n",
        "    if not all(x == gsize.iat[0] for x in gsize):\n",
        "      raise ValueError(\"Not all graphs have the same set of edges. This case is not implemented.\")\n",
        "\n",
        "    # Take any graph from W to find its edges\n",
        "    gid = W.iloc[0].at['graph_ind']\n",
        "    W1 = W[W['graph_ind'] == gid]\n",
        "    self.edges = list(zip(W1['source'], W1['target']))\n",
        "    self.nodes = node_features.index\n",
        "    self.ngraphs = W.groupby('graph_ind').ngroups\n",
        "    self.model = model\n",
        "\n",
        "  def _null_edge_graphs(self, val=0):\n",
        "    \"\"\" Generator of StellarGraph graphs with exactly one edge set to 'val' (default 0)\n",
        "    \"\"\"\n",
        "    for src, tar in self.edges:\n",
        "      cond = (self._W['source'] == src) & (self._W['target'] == tar)\n",
        "      W2 = self._W.copy()\n",
        "      W2['weight'].mask(cond, val, inplace=True) # set weights corresonding to edge to 0\n",
        "      for _, g in W2.groupby('graph_ind'):\n",
        "        yield StellarGraph(nodes=self._node_features, edges=g.drop(columns='graph_ind'),\n",
        "                           node_type_default='microbe', edge_type_default='correlation')\n",
        "\n",
        "\n",
        "  def _null_node_graphs(self):\n",
        "    \"\"\" Generator of StellarGraph graphs with all edges incident to a node set to 0\n",
        "    \"\"\"\n",
        "    for n in self.nodes:\n",
        "      cond = (self._W['source'] == n) | (self._W['target'] == n)\n",
        "      W2 = self._W.copy()\n",
        "      W2['weight'].mask(cond, 0, inplace=True)\n",
        "      for _, g in W2.groupby('graph_ind'):\n",
        "        yield StellarGraph(nodes=self._node_features, edges=g.drop(columns='graph_ind'),\n",
        "                           node_type_default='microbe', edge_type_default='correlation')\n",
        "        \n",
        "        \n",
        "  def _null_2nodes_graphs(self):\n",
        "    \"\"\"Generator of StellarGraph graphs with all edges incident to two nodes set to 0\n",
        "    \"\"\"\n",
        "    for n1, n2 in self.edges:\n",
        "      cond1 = (self._W['source'] == n1) | (self._W['target'] == n1)\n",
        "      cond2 = (self._W['source'] == n2) | (self._W['target'] == n2)\n",
        "      W2 = self._W.copy()\n",
        "      W2['weight'].mask(cond1 | cond2, 0, inplace=True)\n",
        "      for _, g in W2.groupby('graph_ind'):\n",
        "        yield StellarGraph(nodes=self._node_features, edges=g.drop(columns='graph_ind'),\n",
        "                           node_type_default='microbe', edge_type_default='correlation')\n",
        "        \n",
        "  def _null_nnodes_graphs(self, nlist):\n",
        "    \"\"\" Generator of StellarGraph graphs with all edges incident to n nodes set to 0,\n",
        "        Assume the first n-1 nodes are given so only the n'th node need chosen\n",
        "    \"\"\"\n",
        "    from functools import reduce\n",
        "    import operator\n",
        "\n",
        "    if not set(nlist).issubset(self.nodes):\n",
        "      raise ValueError(\"Not all provided nodes are found in the graph\")\n",
        "\n",
        "    conds = [(self._W['source'] == nd) | (self._W['target'] == nd) for nd in nlist]\n",
        "    for n in self.nodes:\n",
        "      if n in nlist:\n",
        "        continue\n",
        "      combined_cond = conds + [(self._W['source'] == n) | (self._W['target'] == n)]\n",
        "      reduced_cond = reduce(operator.or_, combined_cond)\n",
        "      W2 = self._W.copy()\n",
        "      W2['weight'].mask(reduced_cond, 0, inplace=True)\n",
        "      for _, g in W2.groupby('graph_ind'):\n",
        "        yield StellarGraph(nodes=self._node_features, edges=g.drop(columns='graph_ind'),\n",
        "                           node_type_default='microbe', edge_type_default='correlation')\n",
        "    \n",
        "        \n",
        "\n",
        "  @staticmethod\n",
        "  def _batch(iterable, n):\n",
        "    \"\"\" Generate prediction batch of size n, using the grouper idiom \"\"\"\n",
        "    iters = [iter(iterable)] * n\n",
        "    return zip(*iters)\n",
        "\n",
        "  @staticmethod\n",
        "  def compute_lor(pred, P_new):\n",
        "    \"\"\" Compute log-odds ratio between new and original graph predicted probs \n",
        "        Args:\n",
        "            pred: prediction on the original graphs, output of model.predict(),\n",
        "                  shape N-by-1, where N number of graph instances\n",
        "            P_new: predicition on new graphs, shape N-by-K, where K = number of\n",
        "                   edges/nodes depending on edge or node importance \n",
        "        Returns:\n",
        "            numpy array same shape as P_new\n",
        "    \"\"\"\n",
        "    eps = 1e-6\n",
        "    lo1 = np.log(P_new+eps) - np.log(1-P_new+eps)\n",
        "    lo2 = np.log(pred+eps) - np.log(1-pred+eps)\n",
        "    return lo1 - lo2\n",
        "\n",
        "  def read_sg(self):\n",
        "    \"\"\" Read graphs into list of StellarGraph instances \"\"\"\n",
        "    out = list()\n",
        "    for _,g in self._W.groupby('graph_ind'):\n",
        "      out.append(StellarGraph(nodes=self._node_features,     \n",
        "                              edges=g.drop(columns='graph_ind'),\n",
        "                              node_type_default='microbe',\n",
        "                              edge_type_default='correlation')) \n",
        "    return out \n",
        "\n",
        "  def predict_graph(self, graphs):\n",
        "    \"\"\"Use the model to predict the probability of positive class\n",
        "       Args:\n",
        "          graphs: list of StellarGraph graph instances\n",
        "    \"\"\"\n",
        "    fl = PaddedGraphGenerator(graphs=graphs).flow(range(len(graphs)), \n",
        "                                                  batch_size=len(graphs), \n",
        "                                                  symmetric_normalization=False, \n",
        "                                                  weighted=True)\n",
        "    return self.model.predict(fl)\n",
        "\n",
        "  def edge_imp(self, set_wt=0):\n",
        "    \"\"\"Calclate edge importance by log-odds ratio of the probability of the class \n",
        "       when an edge weight is set to 'set_wt' (default 0), to that of the original graph instance\n",
        "    \"\"\"\n",
        "    sg = self.read_sg()\n",
        "    pred = self.predict_graph(sg)\n",
        "    P_new = np.empty((self.ngraphs, len(self.edges)))\n",
        "    \n",
        "    gen = self._null_edge_graphs(set_wt)\n",
        "    for i, bch in enumerate(ImportanceDGCNN._batch(gen, self.ngraphs)):\n",
        "      pred_new = self.predict_graph(list(bch)).reshape(-1)\n",
        "      P_new[:,i] = pred_new\n",
        "      if i % 100 == 0: print(f'{i}: EDGE {self.edges[i]} DONE.')\n",
        "\n",
        "    LR = ImportanceDGCNN.compute_lor(pred, P_new)\n",
        "    stats = self.summary_stats(LR, 'edge')\n",
        "    self.LR_edge, self.LR_edge_stats = LR, stats\n",
        "    return stats, LR\n",
        "\n",
        "  def node_imp(self):\n",
        "    \"\"\"Calclate node importance by log-odds ratio of the probability of the class \n",
        "       when all incident edge weights are set to zero for that node, to that of the \n",
        "       original graph instance\n",
        "    \"\"\"\n",
        "    sg = self.read_sg()\n",
        "    pred = self.predict_graph(sg)\n",
        "    P_new = np.empty((self.ngraphs, len(self.nodes)))\n",
        "\n",
        "    gen = self._null_node_graphs()\n",
        "    for i, bch in enumerate(ImportanceDGCNN._batch(gen, self.ngraphs)):\n",
        "      pred_new = self.predict_graph(list(bch)).reshape(-1)\n",
        "      P_new[:,i] = pred_new\n",
        "\n",
        "    LR = ImportanceDGCNN.compute_lor(pred, P_new)\n",
        "    stats = self.summary_stats(LR, 'node')\n",
        "    self.LR_node, self.LR_node_stats = LR, stats\n",
        "    return stats, LR\n",
        "\n",
        "  def node_pair_imp(self):\n",
        "    \"\"\"Calculate node pair importance by knocking out each pair of nodes\n",
        "    \"\"\"\n",
        "    sg = self.read_sg()\n",
        "    pred = self.predict_graph(sg)\n",
        "    P_new = np.empty((self.ngraphs, len(self.edges)))\n",
        "\n",
        "    gen = self._null_2nodes_graphs()\n",
        "    for i, bch in enumerate(ImportanceDGCNN._batch(gen, self.ngraphs)):\n",
        "      pred_new = self.predict_graph(list(bch)).reshape(-1)\n",
        "      P_new[:,i] = pred_new\n",
        "      if i % 100 == 0: print(f'{i}: NODES {self.edges[i][0], self.edges[i][1]} DONE.')\n",
        "\n",
        "    LR = ImportanceDGCNN.compute_lor(pred, P_new)\n",
        "    stats = self.summary_stats(LR, 'node2')\n",
        "    self.LR_node2, self.LR_node2_stats = LR, stats\n",
        "    return stats, LR\n",
        "\n",
        "  def nnode_imp(self, n):\n",
        "    \"\"\"Calculate n-node importance by knocking out n nodes, using a greedy search\n",
        "       strategy where after the first node resulting in maximum change in LR is added,\n",
        "       the node from the remaining node set resulting in maximun change in LR is \n",
        "       added to form 2-node, and it continues until n nodes are added\n",
        "\n",
        "       Returns:\n",
        "            List of tuples. The first component is the names corresponding\n",
        "            to columns of LR, the second component is LR of shape (n_graphs, n_nodes-n+1).\n",
        "            Return all k-node importance from k = 1 to n\n",
        "    \"\"\"\n",
        "    sg = self.read_sg()\n",
        "    pred = self.predict_graph(sg)\n",
        "    n_full = list(self.nodes)\n",
        "    nlist = []\n",
        "\n",
        "    out = [] \n",
        "    for k in range(1, n+1):\n",
        "      P_new = np.empty((self.ngraphs, len(self.nodes)-k+1))\n",
        "      if k == 1:\n",
        "        gen = self._null_node_graphs()\n",
        "      else:\n",
        "        gen = self._null_nnodes_graphs(nlist)\n",
        "\n",
        "      for i, bch in enumerate(ImportanceDGCNN._batch(gen, self.ngraphs)):\n",
        "        pred_new = self.predict_graph(list(bch)).reshape(-1)\n",
        "        P_new[:,i] = pred_new\n",
        "\n",
        "      # Find which node to add to nlist\n",
        "      LR = ImportanceDGCNN.compute_lor(pred, P_new)\n",
        "      maxi = np.argmax(np.abs(np.median(LR, axis=0)))\n",
        "      n_remain = [nn for nn in n_full if nn not in nlist]\n",
        "      out.append(([tuple(nlist + [x]) for x in n_remain], LR))\n",
        "      nlist = nlist + [n_remain[maxi]]\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "  def summary_stats(self, LR, which):\n",
        "    \"\"\" Get mean, median and std err of log-odds ratio \"\"\"\n",
        "    lor_mean, lor_med = np.mean(LR, axis=0), np.median(LR, axis=0)\n",
        "    lor_std = np.std(LR, axis=0)\n",
        "    df = pd.DataFrame({'lor_mean': lor_mean,\n",
        "                       'lor_med': lor_med, \n",
        "                       'std_err': lor_std/np.sqrt(LR.shape[0])})\n",
        "    if which == 'edge':\n",
        "      df['source'] = [e[0] for e in self.edges]\n",
        "      df['target'] = [e[1] for e in self.edges]\n",
        "\n",
        "    if which == 'node':\n",
        "      df['node'] = self.nodes\n",
        "    \n",
        "    if which == 'node2':\n",
        "      df['node1'] = [e[0] for e in self.edges]\n",
        "      df['node2'] = [e[1] for e in self.edges]\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHLeIY7cUnw0"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Write results to Drive\n",
        "def save_imp_res(node_res, edge_res=None, runid='run'):\n",
        "  with open('/content/drive/MyDrive/MB_banocc_dgcnn/'+runid+'/'+runid+'_'+'imp_res.pkl', 'wb') as pickle_out:\n",
        "    pickle.dump(node_res, pickle_out) # serialize node importance result\n",
        "    if edge_res:\n",
        "      pickle.dump(edge_res, pickle_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoWAiyZTwGZ1"
      },
      "source": [
        "# Main script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf-DrJPFwe4g"
      },
      "source": [
        "## 1-node importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uDfnWy2Pq0h"
      },
      "source": [
        "# It may not be possible to finish all runs in one go due to memory constraint in colab\n",
        "runid = ['r8' + x for x in 'aefg']\n",
        "dgcnn_history = {}\n",
        "K = 10 # number of DGCNN runs\n",
        "N_EPOCH = 80 # number of epochs per run\n",
        "sample_frac = 0.4 # fraction of graphs sampled per iter used to calculate importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB33B5cHRN19"
      },
      "source": [
        "for r in runid:\n",
        "  print(f\"PROCESS RUNID {r}: \")\n",
        "  W_mbc = pd.read_csv('/content/drive/MyDrive/MB_banocc_dgcnn/' + r + '/W_mbc.csv', dtype={'graph_ind': 'int'})\n",
        "  W_mbcrc = pd.read_csv('/content/drive/MyDrive/MB_banocc_dgcnn/' + r + '/W_mbcrc.csv', dtype={'graph_ind': 'int'})\n",
        "\n",
        "  nodes = read_graphs2(W_mbc)[0].nodes() # node labels\n",
        "  feature_array = np.identity(len(nodes)) # feature array taken as identity matrix\n",
        "  node_features = sg.IndexedArray(feature_array, index=list(nodes))\n",
        "  cg = read_graphs2(W_mbc, node_features)\n",
        "  crcg = read_graphs2(W_mbcrc, node_features)\n",
        "  graphs = cg + crcg\n",
        "  graph_labels = pd.Series(len(cg) * ['C'] + len(crcg) * ['CRC'])\n",
        "  graph_labels = pd.get_dummies(graph_labels, drop_first=True)\n",
        "\n",
        "  ngraphs = max(W_mbcrc['graph_ind'])\n",
        "  history = []\n",
        "  node_lor = []\n",
        "  for _ in range(K):\n",
        "    model, hist = train_dgcnn(graphs, graph_labels, n_epochs=N_EPOCH)\n",
        "    history.append(hist)\n",
        "\n",
        "    # Sample a subset of graphs from W_mbcrc\n",
        "    rind = 1 + np.random.choice(ngraphs, size=round(ngraphs*sample_frac), replace=False)\n",
        "    W_mbcrc2 = W_mbcrc[W_mbcrc['graph_ind'].isin(rind)]\n",
        "    imp_calculator = ImportanceDGCNN(W_mbcrc2, node_features, model)\n",
        "    _, LR = imp_calculator.node_imp() # 1-node imp\n",
        "    node_lor.append(LR) \n",
        "\n",
        "  # Merge arrays of log-odds ratio from all K iteration outputs, stats data frame from each\n",
        "  # individual iteration disgarded, but keep node labels for reference\n",
        "  node_res = (list(nodes), np.vstack(node_lor))\n",
        "  dgcnn_history[r] = history\n",
        "  save_imp_res(node_res, runid=r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhykLaj_wZXR"
      },
      "source": [
        "## n-node importance (greedy search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdQmyXFcw0W3"
      },
      "source": [
        "runid = ['r8' + x for x in 'aefg']\n",
        "dgcnn_history = {}\n",
        "K = 10 # number of DGCNN runs\n",
        "N_EPOCH = 80 # number of epochs per run\n",
        "sample_frac = 0.4 # fraction of graphs sampled per iter used to calculate importance\n",
        "n = 20 # n-node importance is desired"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOAMIjYywb57"
      },
      "source": [
        "for r in runid:\n",
        "  print(f\"PROCESS RUNID {r}: \")\n",
        "  W_mbc = pd.read_csv('/content/drive/MyDrive/MB_banocc_dgcnn/' + r + '/W_mbc.csv', dtype={'graph_ind': 'int'})\n",
        "  W_mbcrc = pd.read_csv('/content/drive/MyDrive/MB_banocc_dgcnn/' + r + '/W_mbcrc.csv', dtype={'graph_ind': 'int'})\n",
        "\n",
        "  nodes = read_graphs2(W_mbc)[0].nodes() # node labels\n",
        "  feature_array = np.identity(len(nodes)) # feature array taken as identity matrix\n",
        "  node_features = sg.IndexedArray(feature_array, index=list(nodes))\n",
        "  cg = read_graphs2(W_mbc, node_features)\n",
        "  crcg = read_graphs2(W_mbcrc, node_features)\n",
        "  graphs = cg + crcg\n",
        "  graph_labels = pd.Series(len(cg) * ['C'] + len(crcg) * ['CRC'])\n",
        "  graph_labels = pd.get_dummies(graph_labels, drop_first=True)\n",
        "\n",
        "  ngraphs = max(W_mbcrc['graph_ind'])\n",
        "  history = []\n",
        "  node_lor = []\n",
        "  for _ in range(K):\n",
        "    model, hist = train_dgcnn(graphs, graph_labels, n_epochs=N_EPOCH)\n",
        "    history.append(hist)\n",
        "\n",
        "    # Sample a subset of graphs from W_mbcrc\n",
        "    rind = 1 + np.random.choice(ngraphs, size=round(ngraphs*sample_frac), replace=False)\n",
        "    W_mbcrc2 = W_mbcrc[W_mbcrc['graph_ind'].isin(rind)]\n",
        "    imp_calculator = ImportanceDGCNN(W_mbcrc2, node_features, model)\n",
        "    nn_imp = imp_calculator.nnode_imp(n) # n-node imp\n",
        "  \n",
        "    # Each model can result in different selected k-nodes (k=1,...,n), so will not\n",
        "    # attempt to combine the log-odds ratio arrays\n",
        "    node_lor.append(nn_imp) \n",
        "\n",
        "  dgcnn_history[r] = history\n",
        "  save_imp_res(node_lor, runid=r)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}